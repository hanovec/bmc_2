{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/ooKvUHIGSBihbmptXRuV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanovec/bmc_2/blob/main/Business_Model_Canvas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "GOUcFrv-FFyX",
        "outputId": "53f29c9f-52f8-417c-ebeb-660d9ca6aaaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting API Key Configuration...\n",
            "API Key successfully loaded from Google Colab Secrets.\n",
            "Google Generative AI API configured.\n",
            "\n",
            "Searching for an available Gemini model...\n",
            "  > Found priority model: models/gemini-2.5-flash-preview-05-20\n",
            "Model 'models/gemini-2.5-flash-preview-05-20' initialized successfully.\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Setup, Configuration, and Initialization\n",
        "\n",
        "# Library installations\n",
        "!pip install -q google-generativeai\n",
        "!pip install -q python-dotenv\n",
        "\n",
        "import os\n",
        "import time\n",
        "import google.generativeai as genai\n",
        "import json\n",
        "import IPython\n",
        "\n",
        "# --- Configuration Section ---\n",
        "\n",
        "# Model Configuration\n",
        "# A prioritized list of model name \"stems\" to search for.\n",
        "# The script will use the first one it finds available.\n",
        "# CORRECTED: Your specific model is now the top priority.\n",
        "PRIORITY_MODEL_STEMS = [\n",
        "    \"gemini-2.5-flash-preview-05-20\",  # Your specifically requested preview model.\n",
        "    \"gemini-1.5-flash-latest\",          # Fallback 1: A stable, recent Flash model\n",
        "    \"gemini-1.5-pro-latest\",            # Fallback 2: A recent, powerful Pro model\n",
        "    \"gemini-pro\",                       # Fallback 3: A widely available Pro model\n",
        "]\n",
        "\n",
        "# Generation & Safety Configuration\n",
        "GENERATION_CONFIG = {\n",
        "    \"temperature\": 1.5,\n",
        "    \"top_p\": 0.95,\n",
        "    \"max_output_tokens\": 65536,\n",
        "}\n",
        "\n",
        "SAFETY_SETTINGS = [\n",
        "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "]\n",
        "# --- End of Configuration ---\n",
        "\n",
        "\n",
        "# --- Authentication and Initialization ---\n",
        "model = None # Initialize model to None to ensure it exists\n",
        "print(\"Starting API Key Configuration...\")\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    print(\"API Key successfully loaded from Google Colab Secrets.\")\n",
        "except ImportError:\n",
        "    print(\"Not in Google Colab. Attempting to load API Key from environment variable...\")\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"Google API Key not found. Please set it up in Colab Secrets or as an environment variable.\")\n",
        "else:\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(\"Google Generative AI API configured.\")\n",
        "\n",
        "try:\n",
        "    print(\"\\nSearching for an available Gemini model...\")\n",
        "    model_name_to_use = None\n",
        "    available_models = [m for m in genai.list_models() if \"generateContent\" in m.supported_generation_methods]\n",
        "\n",
        "    for model_stem in PRIORITY_MODEL_STEMS:\n",
        "        # Find the first available model that contains the priority stem and isn't a vision model\n",
        "        found_model = next((m for m in available_models if model_stem in m.name and 'vision' not in m.name.lower()), None)\n",
        "        if found_model:\n",
        "            model_name_to_use = found_model.name\n",
        "            print(f\"  > Found priority model: {model_name_to_use}\")\n",
        "            break\n",
        "\n",
        "    if not model_name_to_use:\n",
        "        raise ValueError(\"Could not find any of the priority models. Please check available models in your region and ensure your project has access to any requested preview models.\")\n",
        "\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=model_name_to_use,\n",
        "        generation_config=GENERATION_CONFIG,\n",
        "        safety_settings=SAFETY_SETTINGS\n",
        "    )\n",
        "    print(f\"Model '{model_name_to_use}' initialized successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"CRITICAL ERROR during model initialization: {e}\")\n",
        "    print(\"Please check your API key, model availability in your region, and project quotas.\")\n",
        "    # The 'model' variable will remain None, which will be caught by checks in other cells."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Helper Function to Interact with Gemini (Simplified)\n",
        "\n",
        "def ask_gemini_sdk(prompt_text: str, temperature: float = None) -> str:\n",
        "    \"\"\"\n",
        "    Sends a prompt to the initialized Gemini model and returns the text response.\n",
        "    Allows for correctly overriding the default temperature for a specific call.\n",
        "    \"\"\"\n",
        "    if not model:\n",
        "        return \"AI_ERROR: Model not initialized. Please ensure Cell 1 executed correctly.\"\n",
        "\n",
        "    config_overrides = {}\n",
        "    if temperature is not None:\n",
        "        config_overrides['temperature'] = float(temperature)\n",
        "        display_status_message(f\"AI is thinking with custom temperature: {config_overrides['temperature']}...\")\n",
        "    else:\n",
        "        display_status_message(\"AI is thinking with default temperature...\")\n",
        "\n",
        "    try:\n",
        "        # The prompt_content is now just the text, no files.\n",
        "        response = model.generate_content(prompt_text, generation_config=config_overrides)\n",
        "\n",
        "        if response.parts:\n",
        "            return response.text.strip()\n",
        "        elif response.prompt_feedback and response.prompt_feedback.block_reason:\n",
        "            reason = response.prompt_feedback.block_reason.name\n",
        "            return f\"AI_ERROR: Your prompt was blocked for safety reasons ({reason}). Please rephrase your input.\"\n",
        "        else:\n",
        "            # Handle other cases like unexpected stops or empty responses\n",
        "            return \"AI_ERROR: Received an incomplete response from the model. Please try again.\"\n",
        "    except Exception as e:\n",
        "        display_status_message(f\"ERROR during API call: {e}\")\n",
        "        return f\"AI_ERROR: An unexpected error occurred: {type(e).__name__}. Please check console.\""
      ],
      "metadata": {
        "id": "XTUdUaWWO74J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Phase 1 - Welcome and Introduction (Streamlined)\n",
        "\n",
        "def display_welcome_and_introduction():\n",
        "    \"\"\"\n",
        "    Displays a single, comprehensive welcome message using a styled box without any pauses.\n",
        "    \"\"\"\n",
        "    welcome_text = (\n",
        "        \"Welcome to the BMC Navigator! I'm your AI business coach, here to help you map and \"\n",
        "        \"innovate your IT business model using the powerful Business Model Canvas (BMC). \"\n",
        "        \"We will begin by uploading your methodology document(s), which will guide our entire analysis.\"\n",
        "    )\n",
        "    ai_box(welcome_text, title=\"üöÄ Welcome Aboard!\")"
      ],
      "metadata": {
        "id": "aq3QZadePHDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Phase 2 - Input Gathering and Validation (with Comprehensive Sub-points Display)\n",
        "\n",
        "# LLM_BMC_HELPER_PERSONA prompt remains the same, it's used for short clarifications.\n",
        "\n",
        "def get_user_input_with_llm_validation(bmc_block_question: str, block_name: str, coverage_points: list = None, it_examples: list[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Prompts the user with a main question and a checklist of points to cover.\n",
        "    Uses styled boxes and Gemini for clarification if the input is vague.\n",
        "    \"\"\"\n",
        "    # IMPROVEMENT: Build a rich HTML prompt that includes the coverage points as a checklist.\n",
        "    full_prompt_html = f\"<p style='font-size: 1.1em; margin-bottom: 15px;'>{bmc_block_question}</p>\"\n",
        "\n",
        "    if coverage_points:\n",
        "        full_prompt_html += \"<p style='margin-bottom: 5px; font-weight: bold;'>Pro komplexn√≠ odpovƒõƒè zva≈æte pros√≠m n√°sleduj√≠c√≠ body:</p>\"\n",
        "        full_prompt_html += \"<ul style='margin-top: 5px; margin-left: 20px;'>\"\n",
        "        for point in coverage_points:\n",
        "            full_prompt_html += f\"<li style='margin-bottom: 5px;'>{point}</li>\"\n",
        "        full_prompt_html += \"</ul>\"\n",
        "\n",
        "    if it_examples:\n",
        "        full_prompt_html += f\"<p style='margin-top: 15px; font-style: italic; color: #555;'>Nap≈ô√≠klad: {', '.join(it_examples)}.</p>\"\n",
        "\n",
        "\n",
        "    while True:\n",
        "        # Step 1: Ask the user for input using the rich HTML prompt\n",
        "        user_response = user_prompt_box(full_prompt_html)\n",
        "        user_response_stripped = user_response.strip()\n",
        "\n",
        "        # Step 2: Immediately redisplay their answer\n",
        "        display_user_response(user_response_stripped)\n",
        "\n",
        "        # Step 3: Validation logic (remains the same)\n",
        "        if user_response_stripped.lower() in [\"n/a\", \"none\", \"skip\"]:\n",
        "            ai_box(f\"Understood. We'll skip '{block_name}' for now.\", title=\"‚úÖ Acknowledged\")\n",
        "            return \"Skipped\"\n",
        "\n",
        "        if len(user_response_stripped) < 25: # Slightly increased threshold for comprehensive answers\n",
        "            ai_box(f\"Dƒõkuji. To je dobr√Ω zaƒç√°tek, ale zkusme p≈ôidat v√≠ce detail≈Ø k jednotliv√Ωm bod≈Øm.\", title=\" digging deeper...\")\n",
        "            # The clarification logic can remain simple, as the user already has the detailed prompt.\n",
        "            # No need to call the LLM again here unless absolutely necessary.\n",
        "        else:\n",
        "            return user_response_stripped"
      ],
      "metadata": {
        "id": "RDmnxKAWPIeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Phase 2 - AI-Driven Question Plan Generation\n",
        "\n",
        "# IMPROVEMENT: The prompt now instructs the AI to use its own deep knowledge of the standard BMC methodology.\n",
        "LLM_EXPERT_QUESTION_PLANNER = \"\"\"\n",
        "You are an expert strategy consultant and a master of the Alex Osterwalder Business Model Canvas methodology. Your task is to create a structured, comprehensive questioning plan to guide a user through a deep-dive description of their IT Reseller/System Integrator business.\n",
        "\n",
        "Your output MUST be a valid JSON list of 9 objects, one for each core block of the Business Model Canvas. The order should be logical (Customers/Value -> Operations -> Finances). Each object must have the following four keys:\n",
        "1. \"key\": The standard snake_case identifier for the block (e.g., \"customer_segments\").\n",
        "2. \"question\": The main, user-friendly question for the block.\n",
        "3. \"coverage_points\": A list of 3-4 critical sub-questions or topics the user MUST consider to provide a complete answer for that block. These should be insightful and cover the nuances of the methodology.\n",
        "4. \"examples\": A list of 3-4 short, relevant examples for an IT Reseller/System Integrator.\n",
        "\n",
        "Example of one object in the list:\n",
        "{\n",
        "  \"key\": \"value_propositions\",\n",
        "  \"question\": \"Now, let's detail your Value Propositions. What value do you deliver to your customers?\",\n",
        "  \"coverage_points\": [\n",
        "    \"Which specific customer problem are you solving or which need are you satisfying?\",\n",
        "    \"What bundle of products and services are you offering to each segment?\",\n",
        "    \"How does your offering differ from competitors (e.g., is it about performance, price, design, convenience)?\",\n",
        "    \"Are you offering something new and disruptive, or improving an existing solution?\"\n",
        "  ],\n",
        "  \"examples\": [\"Managed Cybersecurity (SOC-as-a-Service)\", \"Custom Cloud Migration Projects\", \"Hardware procurement with expert lifecycle advice\", \"24/7 Premium Technical Support\"]\n",
        "}\n",
        "\n",
        "Generate ONLY the JSON list and nothing else.\n",
        "\"\"\"\n",
        "\n",
        "def generate_question_plan() -> list:\n",
        "    \"\"\"\n",
        "    Uses Gemini to generate a dynamic and comprehensive list of questions based on its internal\n",
        "    knowledge of the Business Model Canvas methodology.\n",
        "    \"\"\"\n",
        "    ai_box(\"Jsem expert na metodologii Business Model Canvas. P≈ôipravuji pro v√°s personalizovan√Ω pl√°n dotazov√°n√≠, abychom prozkoumali v√°≈° byznys do hloubky.\", title=\"üß† P≈ô√≠prava Pl√°nu\")\n",
        "\n",
        "    # The prompt is now self-contained, no files needed.\n",
        "    response_text = ask_gemini_sdk(LLM_EXPERT_QUESTION_PLANNER, temperature=0.2)\n",
        "\n",
        "    if \"AI_ERROR\" in response_text:\n",
        "        ai_box(\"Nepoda≈ôilo se mi vytvo≈ôit pl√°n. Zkuste pros√≠m spustit bu≈àku znovu.\", title=\"‚ùå Chyba Pl√°nu\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        cleaned_json_text = response_text.strip().lstrip(\"```json\").rstrip(\"```\").strip()\n",
        "        question_plan = json.loads(cleaned_json_text)\n",
        "        if isinstance(question_plan, list) and all('key' in item and 'question' in item and 'coverage_points' in item for item in question_plan):\n",
        "            ai_box(f\"Pl√°n dotazov√°n√≠ byl √∫spƒõ≈°nƒõ vygenerov√°n. Zept√°m se v√°s na {len(question_plan)} kl√≠ƒçov√Ωch oblast√≠.\", title=\"‚úÖ Pl√°n P≈ôipraven\")\n",
        "            return question_plan\n",
        "        else:\n",
        "            raise ValueError(\"Vygenerovan√Ω JSON postr√°d√° po≈æadovan√© kl√≠ƒçe.\")\n",
        "    except (json.JSONDecodeError, ValueError) as e:\n",
        "        ai_box(f\"Nastala chyba p≈ôi zpracov√°n√≠ vygenerovan√©ho pl√°nu: {e}.\", title=\"‚ùå Chyba Zpracov√°n√≠\")\n",
        "        return []\n",
        "\n",
        "# The conduct_dynamic_bmc_analysis function from the previous version is perfect and needs NO changes.\n",
        "# We will just copy it here to keep Cell 5 self-contained.\n",
        "def conduct_dynamic_bmc_analysis(question_plan: list) -> dict:\n",
        "    ai_box(\"Nyn√≠ spoleƒçnƒõ projdeme jednotliv√© bloky va≈°eho byznys modelu do hloubky.\", title=\"üöÄ Jdeme na to!\")\n",
        "    bmc_data = {}\n",
        "    for i, config in enumerate(question_plan):\n",
        "        display_status_message(f\"Oblast {i+1} z {len(question_plan)}: {config.get('key', 'Nezn√°m√Ω blok').replace('_', ' ').title()}\")\n",
        "        response = get_user_input_with_llm_validation(\n",
        "            bmc_block_question=config.get('question', 'Chyb√≠ text ot√°zky.'),\n",
        "            block_name=config.get('key', f'Ot√°zka {i+1}'),\n",
        "            coverage_points=config.get('coverage_points', []),\n",
        "            it_examples=config.get('examples', [])\n",
        "        )\n",
        "        bmc_data[config.get('key', f'custom_question_{i+1}')] = response\n",
        "    ai_box(\"Skvƒõl√° pr√°ce! Zmapovali jsme cel√Ω v√°≈° byznys model.\", title=\"üéâ Hotovo!\")\n",
        "    return bmc_data"
      ],
      "metadata": {
        "id": "fweV9i48PKTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Phase 3 - LLM-Powered Input Analysis\n",
        "# ... (LLM_DEEP_ANALYSIS_PERSONA_V2 prompt remains the same) ...\n",
        "\n",
        "def perform_llm_bmc_analysis(bmc_data: dict) -> str:\n",
        "    # This function no longer needs the file_refs parameter\n",
        "    display_status_message(\"Initiating expert strategic analysis...\")\n",
        "    bmc_data_string = \"\\n\".join([f\"- {key}: {value}\" for key, value in bmc_data.items() if value != \"Skipped\"])\n",
        "    analysis_prompt = f\"{LLM_DEEP_ANALYSIS_PERSONA_V2}\\n\\nHere is the BMC data from the user:\\n{bmc_data_string}\"\n",
        "    return ask_gemini_sdk(analysis_prompt, temperature=0.8)"
      ],
      "metadata": {
        "id": "USDJMMVpPMCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Phase 4 - LLM-Powered Suggestion Generation\n",
        "# ... (LLM_INNOVATION_SUGGESTION_PERSONA_V2 prompt remains the same) ...\n",
        "\n",
        "def generate_llm_suggestions(bmc_data_str: str, analysis_summary: str) -> str:\n",
        "    # This function no longer needs the file_refs parameter\n",
        "    display_status_message(\"Generating innovation proposals based on expert analysis...\")\n",
        "    suggestion_prompt = f\"\"\"...\"\"\" # The prompt content is the same\n",
        "    return ask_gemini_sdk(suggestion_prompt, temperature=1.2)"
      ],
      "metadata": {
        "id": "iRbl-IXtaP3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Main Execution Flow (Simplified - No File Upload)\n",
        "\n",
        "# ... (display_llm_output helper function remains the same) ...\n",
        "\n",
        "def run_main_session():\n",
        "    \"\"\"Orchestrates the entire AI-driven BMC Navigator session.\"\"\"\n",
        "    if not model:\n",
        "        ai_box(\"Gemini model was not initialized. The session cannot start.\", title=\"KRITICK√Å CHYBA\")\n",
        "        return\n",
        "\n",
        "    # Phase 1: Welcome and Plan Generation\n",
        "    display_welcome_and_introduction() # Shows the welcome message\n",
        "    question_plan = generate_question_plan() # Immediately generates the plan\n",
        "\n",
        "    if not question_plan:\n",
        "        ai_box(\"Nepoda≈ôilo se mi p≈ôipravit pl√°n dotazov√°n√≠. Zkuste pros√≠m spustit sezen√≠ znovu.\", title=\"‚ùå Chyba Spu≈°tƒõn√≠\")\n",
        "        return\n",
        "\n",
        "    # Phase 2: Dynamic Questioning\n",
        "    current_bmc_data = conduct_dynamic_bmc_analysis(question_plan)\n",
        "\n",
        "    # Phase 3: Analysis\n",
        "    analysis_result = perform_llm_bmc_analysis(current_bmc_data)\n",
        "    display_llm_output(\"F√°ze 3: Strategick√° Anal√Ωza\", analysis_result)\n",
        "    input(\"U≈æivatel (Stisknƒõte Enter pro pokraƒçov√°n√≠ k N√°vrh≈Øm Inovac√≠): \")\n",
        "\n",
        "    # Phase 4: Suggestions\n",
        "    bmc_summary_str = \"\\n\".join([f\"- {k}: {v}\" for k, v in current_bmc_data.items() if v != \"Skipped\"])\n",
        "    suggestions_result = generate_llm_suggestions(bmc_summary_str, analysis_result)\n",
        "    display_llm_output(\"F√°ze 4: N√°vrhy Inovac√≠\", suggestions_result)\n",
        "\n",
        "    # Conclusion\n",
        "    ai_box(\n",
        "        \"T√≠mto konƒç√≠ na≈°e interaktivn√≠ sezen√≠ s BMC Navig√°torem. Anal√Ωza a n√°vrhy byly zalo≈æeny na expertn√≠ znalosti standardn√≠ Business Model Canvas metodologie.\",\n",
        "        title=\"üéâ Sezen√≠ Dokonƒçeno!\"\n",
        "    )\n",
        "\n",
        "# --- Spu≈°tƒõn√≠ Hlavn√≠ho Sezen√≠ ---\n",
        "if __name__ == \"__main__\":\n",
        "    run_main_session()"
      ],
      "metadata": {
        "id": "bOE5epywPPxO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "d27e7a85-60be-4167-ee34-acc08a609c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div style=\"border: 2px solid #4A90E2; border-radius: 10px; padding: 15px; margin: 10px 0; background-color: #F0F7FF; box-shadow: 2px 2px 5px rgba(0,0,0,0.1);\">\n",
              "        <p style=\"margin: 0; padding: 0; font-weight: bold; color: #4A90E2; font-family: sans-serif;\">üöÄ Welcome Aboard!</p>\n",
              "        <hr style=\"border: 0; border-top: 1px solid #D0E0F0; margin: 10px 0;\">\n",
              "        <p style=\"margin: 0; padding: 0; font-family: sans-serif; color: #333;\">Welcome to the BMC Navigator! I'm your AI business coach, here to help you map and innovate your IT business<br>model using the powerful Business Model Canvas (BMC). We will begin by uploading your methodology document(s),<br>which will guide our entire analysis.</p>\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div style=\"border: 2px solid #4A90E2; border-radius: 10px; padding: 15px; margin: 10px 0; background-color: #F0F7FF; box-shadow: 2px 2px 5px rgba(0,0,0,0.1);\">\n",
              "        <p style=\"margin: 0; padding: 0; font-weight: bold; color: #4A90E2; font-family: sans-serif;\">üß† P≈ô√≠prava Pl√°nu</p>\n",
              "        <hr style=\"border: 0; border-top: 1px solid #D0E0F0; margin: 10px 0;\">\n",
              "        <p style=\"margin: 0; padding: 0; font-family: sans-serif; color: #333;\">Jsem expert na metodologii Business Model Canvas. P≈ôipravuji pro v√°s personalizovan√Ω pl√°n dotazov√°n√≠, abychom<br>prozkoumali v√°≈° byznys do hloubky.</p>\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'display_status_message' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-80-2841687587.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# --- Spu≈°tƒõn√≠ Hlavn√≠ho Sezen√≠ ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mrun_main_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-80-2841687587.py\u001b[0m in \u001b[0;36mrun_main_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Phase 1: Welcome and Plan Generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdisplay_welcome_and_introduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Shows the welcome message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mquestion_plan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_question_plan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Immediately generates the plan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mquestion_plan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-77-1869575126.py\u001b[0m in \u001b[0;36mgenerate_question_plan\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# The prompt is now self-contained, no files needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mresponse_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mask_gemini_sdk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLLM_EXPERT_QUESTION_PLANNER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"AI_ERROR\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-74-4079106981.py\u001b[0m in \u001b[0;36mask_gemini_sdk\u001b[0;34m(prompt_text, temperature)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mconfig_overrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temperature'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mdisplay_status_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"AI is thinking with custom temperature: {config_overrides['temperature']}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdisplay_status_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AI is thinking with default temperature...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'display_status_message' is not defined"
          ]
        }
      ]
    }
  ]
}