# bmc_navigator.py
#
# Jednosouborov√° verze Business Model Canvas Navig√°toru.
# Pro spu≈°tƒõn√≠ je nutn√© m√≠t ve stejn√©m adres√°≈ôi soubor `.env` s GOOGLE_API_KEY
# a nainstalovan√© knihovny z `requirements.txt`.

import os
import json
import textwrap
import google.generativeai as genai
from dotenv import load_dotenv

# ==============================================================================
# --- KONFIGURAƒåN√ç SEKCE ---
# Parametry jsou zachov√°ny p≈ôesnƒõ podle p≈Øvodn√≠ho Colab notebooku.
# ==============================================================================

# Prioritizovan√Ω seznam model≈Ø
PRIORITY_MODEL_STEMS = [
    "gemini-2.5-flash-preview-05-20",  # Specifick√Ω preview model
    "gemini-1.5-flash-latest",          # Fallback 1
    "gemini-1.5-pro-latest",            # Fallback 2
    "gemini-pro",                       # Fallback 3
]

# Konfigurace generov√°n√≠
GENERATION_CONFIG = {
    "temperature": 1.5,
    "top_p": 0.95,
    "max_output_tokens": 65536,
}

# Bezpeƒçnostn√≠ nastaven√≠
SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
]

# --- Prompty pro LLM ---

LLM_EXPERT_QUESTION_PLANNER = """
You are an expert strategy consultant and a master of the Alex Osterwalder Business Model Canvas methodology. Your task is to create a structured, comprehensive questioning plan to guide a user through a deep-dive description of their IT Reseller/System Integrator business.

Your output MUST be a valid JSON list of 9 objects, one for each core block of the Business Model Canvas. The order should be logical (Customers/Value -> Operations -> Finances). Each object must have the following four keys:
1. "key": The standard snake_case identifier for the block (e.g., "customer_segments").
2. "question": The main, user-friendly question for the block.
3. "coverage_points": A list of 3-4 critical sub-questions or topics the user MUST consider to provide a complete answer for that block. These should be insightful and cover the nuances of the methodology.
4. "examples": A list of 3-4 short, relevant examples for an IT Reseller/System Integrator.

Example of one object in the list:
{
  "key": "value_propositions",
  "question": "Now, let's detail your Value Propositions. What value do you deliver to your customers?",
  "coverage_points": [
    "Which specific customer problem are you solving or which need are you satisfying?",
    "What bundle of products and services are you offering to each segment?",
    "How does your offering differ from competitors (e.g., is it about performance, price, design, convenience)?",
    "Are you offering something new and disruptive, or improving an existing solution?"
  ],
  "examples": ["Managed Cybersecurity (SOC-as-a-Service)", "Custom Cloud Migration Projects", "Hardware procurement with expert lifecycle advice", "24/7 Premium Technical Support"]
}

Generate ONLY the JSON list and nothing else.
"""

# !!! D≈ÆLE≈ΩIT√â: Nahraƒète n√°sleduj√≠c√≠ z√°stupn√© texty va≈°imi skuteƒçn√Ωmi prompty!
LLM_DEEP_ANALYSIS_PERSONA_V2 = """
**[Z√ÅSTUPN√ù TEXT - NAHRAƒéTE SV√ùM SKUTEƒåN√ùM PROMPTEM PRO ANAL√ùZU]**
You are a top-tier business strategist. Analyze the provided Business Model Canvas data for an IT company.
- Identify key strengths and weaknesses in each block.
- Point out potential misalignments between blocks (e.g., value proposition doesn't match customer segment needs).
- Summarize the overall coherence and viability of the business model.
- Present the analysis in a structured, easy-to-read format using Markdown.
"""

LLM_INNOVATION_SUGGESTION_PERSONA_V2 = """
**[Z√ÅSTUPN√ù TEXT - NAHRAƒéTE SV√ùM SKUTEƒåN√ùM PROMPTEM PRO N√ÅVRHY]**
You are a creative innovation consultant specializing in the IT sector. Based on the user's business model data and the strategic analysis, generate actionable, innovative suggestions.
- For each BMC block, provide 1-2 concrete, creative ideas for improvement or new opportunities.
- Ideas should be relevant to an IT Reseller/System Integrator.
- Explain the potential benefit of each suggestion.
- Present the suggestions in a clear, compelling format using Markdown.
"""

# Glob√°ln√≠ promƒõnn√° pro model
model = None

# ==============================================================================
# --- POMOCN√â FUNKCE (UI a API) ---
# ==============================================================================

def ai_box(text: str, title: str):
    """Zobraz√≠ zpr√°vu od AI ve form√°tovan√©m r√°meƒçku v termin√°lu."""
    print("\n" + "="*80)
    print(f"ü§ñ {title.upper()}")
    print("-"*80)
    wrapped_text = textwrap.fill(text, width=78)
    print(wrapped_text)
    print("="*80 + "\n")

def display_user_response(text: str):
    """Zobraz√≠ odpovƒõƒè u≈æivatele v termin√°lu."""
    print("\n" + "-"*80)
    print("üë§ VA≈†E ODPOVƒöƒé:")
    wrapped_text = textwrap.fill(text, width=78)
    print(wrapped_text)
    print("-"*80 + "\n")

def user_prompt_box(main_question: str, coverage_points: list, it_examples: list) -> str:
    """Zobraz√≠ komplexn√≠ ot√°zku a po≈æ√°d√° u≈æivatele o vstup v termin√°lu."""
    print("\n" + "*"*80)
    print(textwrap.fill(f"‚ùì {main_question}", width=78))

    if coverage_points:
        print("\n   Pro komplexn√≠ odpovƒõƒè zva≈æte pros√≠m n√°sleduj√≠c√≠ body:")
        for point in coverage_points:
            print(f"     ‚Ä¢ {textwrap.fill(point, width=72, subsequent_indent='       ')}")

    if it_examples:
        print(f"\n   Nap≈ô√≠klad: {', '.join(it_examples)}.")

    print("*"*80)
    response = input(">> V√°≈° vstup (nebo napi≈°te 'skip' pro p≈ôeskoƒçen√≠): ")
    return response

def display_llm_output(title: str, text: str):
    """Zobraz√≠ fin√°ln√≠ v√Ωstup od LLM (anal√Ωzu, n√°vrhy)."""
    ai_box(text, title)

def display_status_message(text: str):
    """Zobraz√≠ jednoduchou stavovou zpr√°vu."""
    print(f"... [INFO] {text}")

def ask_gemini_sdk(prompt_text: str, temperature: float = None) -> str:
    """Ode≈°le prompt na Gemini model a vr√°t√≠ textovou odpovƒõƒè."""
    global model
    if not model:
        return "AI_ERROR: Model not initialized. Please ensure Cell 1 executed correctly."

    config_overrides = {}
    if temperature is not None:
        config_overrides['temperature'] = float(temperature)
        display_status_message(f"AI is thinking with custom temperature: {config_overrides['temperature']}...")
    else:
        display_status_message("AI is thinking with default temperature...")

    try:
        response = model.generate_content(prompt_text, generation_config=config_overrides)

        if response.parts:
            return response.text.strip()
        elif response.prompt_feedback and response.prompt_feedback.block_reason:
            reason = response.prompt_feedback.block_reason.name
            return f"AI_ERROR: Your prompt was blocked for safety reasons ({reason}). Please rephrase your input."
        else:
            return "AI_ERROR: Received an incomplete response from the model. Please try again."
    except Exception as e:
        display_status_message(f"ERROR during API call: {e}")
        return f"AI_ERROR: An unexpected error occurred: {type(e).__name__}. Please check console."

# ==============================================================================
# --- F√ÅZE APLIKACE ---
# ==============================================================================

def display_welcome_and_introduction():
    """Zobraz√≠ √∫vodn√≠ zpr√°vu."""
    welcome_text = (
        "Welcome to the BMC Navigator! I'm your AI business coach, here to help you map and "
        "innovate your IT business model using the powerful Business Model Canvas (BMC). "
        "We will begin by exploring your business model block by block."
    )
    ai_box(welcome_text, title="üöÄ Welcome Aboard!")

def get_user_input_with_llm_validation(bmc_block_question: str, block_name: str, coverage_points: list, it_examples: list) -> str:
    """Zept√° se u≈æivatele a validuje odpovƒõƒè."""
    while True:
        user_response = user_prompt_box(bmc_block_question, coverage_points, it_examples)
        user_response_stripped = user_response.strip()

        display_user_response(user_response_stripped)

        if user_response_stripped.lower() in ["n/a", "none", "skip"]:
            ai_box(f"Understood. We'll skip '{block_name}' for now.", title="‚úÖ Acknowledged")
            return "Skipped"

        if len(user_response_stripped) < 25:
            ai_box("Dƒõkuji. To je dobr√Ω zaƒç√°tek, ale zkusme p≈ôidat v√≠ce detail≈Ø k jednotliv√Ωm bod≈Øm.", title=" digging deeper...")
        else:
            return user_response_stripped

def generate_question_plan() -> list:
    """Vygeneruje dynamick√Ω pl√°n ot√°zek pomoc√≠ Gemini."""
    ai_box("Jsem expert na metodologii Business Model Canvas. P≈ôipravuji pro v√°s personalizovan√Ω pl√°n dotazov√°n√≠, abychom prozkoumali v√°≈° byznys do hloubky.", title="üß† P≈ô√≠prava Pl√°nu")
    response_text = ask_gemini_sdk(LLM_EXPERT_QUESTION_PLANNER, temperature=0.2)

    if "AI_ERROR" in response_text:
        ai_box(f"Nepoda≈ôilo se mi vytvo≈ôit pl√°n: {response_text}", title="‚ùå Chyba Pl√°nu")
        return []

    try:
        cleaned_json_text = response_text.strip().lstrip("```json").rstrip("```").strip()
        question_plan = json.loads(cleaned_json_text)
        if isinstance(question_plan, list) and all('key' in item and 'question' in item and 'coverage_points' in item for item in question_plan):
            ai_box(f"Pl√°n dotazov√°n√≠ byl √∫spƒõ≈°nƒõ vygenerov√°n. Zept√°m se v√°s na {len(question_plan)} kl√≠ƒçov√Ωch oblast√≠.", title="‚úÖ Pl√°n P≈ôipraven")
            return question_plan
        else:
            raise ValueError("Vygenerovan√Ω JSON postr√°d√° po≈æadovan√© kl√≠ƒçe.")
    except (json.JSONDecodeError, ValueError) as e:
        ai_box(f"Nastala chyba p≈ôi zpracov√°n√≠ vygenerovan√©ho pl√°nu: {e}.", title="‚ùå Chyba Zpracov√°n√≠")
        return []

def conduct_dynamic_bmc_analysis(question_plan: list) -> dict:
    """Provede u≈æivatele sadou dynamicky generovan√Ωch ot√°zek."""
    ai_box("Nyn√≠ spoleƒçnƒõ projdeme jednotliv√© bloky va≈°eho byznys modelu do hloubky.", title="üöÄ Jdeme na to!")
    bmc_data = {}
    for i, config in enumerate(question_plan):
        display_status_message(f"Oblast {i+1} z {len(question_plan)}: {config.get('key', 'Nezn√°m√Ω blok').replace('_', ' ').title()}")
        response = get_user_input_with_llm_validation(
            bmc_block_question=config.get('question', 'Chyb√≠ text ot√°zky.'),
            block_name=config.get('key', f'Ot√°zka {i+1}'),
            coverage_points=config.get('coverage_points', []),
            it_examples=config.get('examples', [])
        )
        bmc_data[config.get('key', f'custom_question_{i+1}')] = response
    ai_box("Skvƒõl√° pr√°ce! Zmapovali jsme cel√Ω v√°≈° byznys model.", title="üéâ Hotovo!")
    return bmc_data

def perform_llm_bmc_analysis(bmc_data: dict) -> str:
    """Ode≈°le sebran√° data k anal√Ωze."""
    display_status_message("Initiating expert strategic analysis...")
    bmc_data_string = "\n".join([f"- {key}: {value}" for key, value in bmc_data.items() if value != "Skipped"])
    analysis_prompt = f"{LLM_DEEP_ANALYSIS_PERSONA_V2}\n\nHere is the BMC data from the user:\n{bmc_data_string}"
    return ask_gemini_sdk(analysis_prompt, temperature=0.8)

def generate_llm_suggestions(bmc_data_str: str, analysis_summary: str) -> str:
    """Po≈æ√°d√° o n√°vrhy na z√°kladƒõ dat a anal√Ωzy."""
    display_status_message("Generating innovation proposals based on expert analysis...")
    suggestion_prompt = (
        f"{LLM_INNOVATION_SUGGESTION_PERSONA_V2}\n\n"
        f"**User's Business Model Canvas Data:**\n{bmc_data_str}\n\n"
        f"**Strategic Analysis Summary:**\n{analysis_summary}\n\n"
        "Now, generate the innovation suggestions based on all the above information."
    )
    return ask_gemini_sdk(suggestion_prompt, temperature=1.2)


# ==============================================================================
# --- HLAVN√ç SPU≈†TƒöC√ç BLOK ---
# ==============================================================================

def run_main_session():
    """≈ò√≠d√≠ cel√Ω interaktivn√≠ proces od zaƒç√°tku do konce."""
    global model

    # F√°ze 1: √övod a generov√°n√≠ pl√°nu
    display_welcome_and_introduction()
    question_plan = generate_question_plan()

    if not question_plan:
        ai_box("Nepoda≈ôilo se mi p≈ôipravit pl√°n dotazov√°n√≠. Zkuste pros√≠m spustit sezen√≠ znovu.", title="‚ùå Chyba Spu≈°tƒõn√≠")
        return

    # F√°ze 2: Dynamick√© dotazov√°n√≠
    current_bmc_data = conduct_dynamic_bmc_analysis(question_plan)

    # F√°ze 3: Anal√Ωza
    analysis_result = perform_llm_bmc_analysis(current_bmc_data)
    display_llm_output("F√°ze 3: Strategick√° Anal√Ωza", analysis_result)
    input("Stisknƒõte Enter pro pokraƒçov√°n√≠ k N√°vrh≈Øm Inovac√≠...")

    # F√°ze 4: N√°vrhy
    bmc_summary_str = "\n".join([f"- {k}: {v}" for k, v in current_bmc_data.items() if v != "Skipped"])
    suggestions_result = generate_llm_suggestions(bmc_summary_str, analysis_result)
    display_llm_output("F√°ze 4: N√°vrhy Inovac√≠", suggestions_result)

    # Z√°vƒõr
    ai_box(
        "T√≠mto konƒç√≠ na≈°e interaktivn√≠ sezen√≠ s BMC Navig√°torem. Anal√Ωza a n√°vrhy byly zalo≈æeny na expertn√≠ znalosti standardn√≠ Business Model Canvas metodologie.",
        title="üéâ Sezen√≠ Dokonƒçeno!"
    )

if __name__ == "__main__":
    # --- Autentizace a Inicializace ---
    print("Starting API Key Configuration...")
    load_dotenv()
    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')

    if not GOOGLE_API_KEY:
        raise ValueError("Google API Key not found. Please create a .env file and set the GOOGLE_API_KEY variable.")
    else:
        genai.configure(api_key=GOOGLE_API_KEY)
        print("Google Generative AI API configured.")

    try:
        print("\nSearching for an available Gemini model...")
        model_name_to_use = None
        available_models = [m for m in genai.list_models() if "generateContent" in m.supported_generation_methods]

        for model_stem in PRIORITY_MODEL_STEMS:
            found_model = next((m for m in available_models if model_stem in m.name and 'vision' not in m.name.lower()), None)
            if found_model:
                model_name_to_use = found_model.name
                print(f"  > Found priority model: {model_name_to_use}")
                break

        if not model_name_to_use:
            raise ValueError("Could not find any of the priority models. Please check available models and project access.")

        model = genai.GenerativeModel(
            model_name=model_name_to_use,
            generation_config=GENERATION_CONFIG,
            safety_settings=SAFETY_SETTINGS
        )
        print(f"Model '{model_name_to_use}' initialized successfully.")

        # --- Spu≈°tƒõn√≠ Hlavn√≠ho Sezen√≠ ---
        run_main_session()

    except Exception as e:
        print(f"\nCRITICAL ERROR during initialization or execution: {e}")
        print("Please check your API key, model availability in your region, and project quotas.")
